[toc]

### 1. 按照主从架构中的主库配置一台新的主库

#### 1）Master1

###### 使用vi /etc/my.cnf命令修改Master配置文件（基于主从架构的模式，添加一些配置）

```properties
#bin_log配置 
log_bin=mysql-bin
server-id=1
# 每次写入,binlog磁盘写入
sync-binlog=1
binlog-ignore-db=information_schema
binlog-ignore-db=mysql
binlog-ignore-db=performance_schema
binlog-ignore-db=sys

# （双主添加）relay_log配置 
relay_log=mysql-relay-bin
log_slave_updates=1 
relay_log_purge=0

# 双主模式中该主库的id偏移量
auto-increment_offset=1
# 双主模式中id的步长
auto-increment_increment=2
```

###### 重启MySQL

```sh
systemctl restart mysqld;
```

###### 主库给从库授权及用户授权

![image-20210922041351728](images/image-20210922041351728.png)

登录MySQL，在MySQL命令行执行如下命令：

```sh
// 授权从库复制
mysql> grant replication slave on *.* to root@'%' identified by '密码'; 

// 授权用户权限
mysql> grant all privileges on *.* to root@'%' identified by '密码'; 
mysql> flush privileges;

//查看主库状态信息，例如master_log_file='mysql-bin.000007',master_log_pos=154 
mysql> show master status;
```



#### 2）Master2

###### 使用vi /etc/my.cnf命令修改Master配置文件（基于主从架构的模式，添加一些配置）

![image-20210922041245603](images/image-20210922041245603.png)

```properties
#bin_log配置 
log_bin=mysql-bin
server-id=3
# 每次写入,binlog磁盘写入
sync-binlog=1
binlog-ignore-db=information_schema
binlog-ignore-db=mysql
binlog-ignore-db=performance_schema
binlog-ignore-db=sys

# （双主添加）relay_log配置 
relay_log=mysql-relay-bin
log_slave_updates=1 
relay_log_purge=0

# 双主模式中该主库的id偏移量
auto-increment_offset=2
# 双主模式中id的步长
auto-increment_increment=2
```

###### 重启MySQL、登陆MySQL授权，与Master1一致。。



#### 3）双主开启同步

两个主库分别配置为对方的从库

![image-20210922042010323](images/image-20210922042010323.png)

登录MySQL，在Slave节点的MySQL命令行执行同步操作，例如下面命令（注意参数与上面showmaster status操作显示的参数一致）：

```sh
change master to master_host='192.168.31.199',master_port=3306,master_user='root',master_password ='123456',master_log_file='mysql-bin.000007',master_log_pos=154; 

start slave; // 开启同步

show slave status \G;
```





### 2.使用MHA实现双机热备

#### 1）四台机器ssh互通

###### 在四台服务器上分别执行下面命令，生成公钥和私钥（注意：连续按换行回车采用默认值）

```sh
ssh-keygen -t rsa
```



###### 在三台MySQL服务器分别执行下面命令，密码输入系统密码，将公钥拷到MHA Manager服务器上

```sh
ssh-copy-id 目标服务器ip
```



###### 之后可以在MHA Manager服务器上检查下，看看.ssh/authorized_keys文件是否包含3个公钥

```sh
cat /root/.ssh/authorized_keys
```



###### 执行下面命令，将MHA Manager的公钥添加到authorized_keys文件中（此时应该包含4个公钥）

```sh
cat /root/.ssh/id_rsa.pub >> /root/.ssh/authorized_keys
```



###### 从MHA Manager服务器执行下面命令，向其他三台MySQL服务器分发公钥信息

```sh
scp /root/.ssh/authorized_keys root@192.168.31.199:/root/.ssh/authorized_keys 
scp /root/.ssh/authorized_keys root@192.168.31.165:/root/.ssh/authorized_keys 
scp /root/.ssh/authorized_keys root@192.168.31.142:/root/.ssh/authorized_keys
```



###### 可以MHA Manager执行下面命令，检测下与三台MySQL是否实现ssh互通

```sh
ssh 192.168.31.199 
exit 

ssh 192.168.31.165 
exit 

ssh 192.168.31.142 
exit
```





#### 2）MHA node 和manager下载

- 三台MySQL服务器需要安装 node

  - Linux在线下载

    wget https://github.com/yoshinorim/mha4mysql-node/releases/download/v0.58/mha4mysql-node-0.58-0.el7.centos.noarch.rpm

  - 本机下载后上传Linux

    https://github.com/yoshinorim/mha4mysql-node/releases/tag/v0.58

- MHA Manager服务器需要安装 manager和node
  - Linux在线下载

    wget https://github.com/yoshinorim/mha4mysql-manager/releases/download/v0.58/mha4mysql-manager-0.58-0.el7.centos.noarch.rpm

  - 本机下载后上传Linux

    https://github.com/yoshinorim/mha4mysql-manager/releases/tag/v0.58







#### 3）MHA node安装

###### 在四台服务器上安装mha4mysql-node，MHA的Node依赖于perl-DBD-MySQL，所以要先安装perl-DBD-MySQL 

```sh
yum install perl-DBD-MySQL -y
```



###### 安装mha4mysql-node

```sh
rpm -ivh mha4mysql-node-0.58-0.el7.centos.noarch.rpm
```







#### 4）MHA manager安装

在MHA Manager服务器已经安装了 mha4mysql-node，再继续安装 mha4mysql-manager，MHA的manager又依赖了perl-Config-Tiny、perl-Log-Dispatch、perl-Parallel-ForkManager，也分别进行安装 

> 提示：由于perl-Log-Dispatch和perl-Parallel-ForkManager这两个被依赖包在yum仓库找不到，
>
> 因此安装epel-release-latest-7.noarch.rpm。在使用时，可能会出现下面异常：**Cannot**
>
> **retrieve metalink for repository: epel/x86_64**。可以尝试使
>
> 用/etc/yum.repos.d/epel.repo，然后注释掉metalink，取消注释baseurl。



###### 安装epel-release-latest-7.noarch.rpm（软件源）

```sh
wget http://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm

rpm -ivh epel-release-latest-7.noarch.rpm
```



###### 安装MHA manager相关依赖

```sh
yum install perl-DBD-MySQL perl-Config-Tiny perl-Log-Dispatch perl-Parallel-ForkManager -y
```



###### 安装mha4mysql-manager

```sh
rpm -ivh mha4mysql-manager-0.58-0.el7.centos.noarch.rpm
```







#### 5）MHA 配置文件

MHA Manager服务器需要为每个监控的 Master/Slave 集群提供一个专用的配置文件，而所有的Master/Slave 集群也可共享全局配置。



###### 初始化配置目录

```sh
#目录说明
#/var/log (CentOS目录)
#        /mha (MHA监控根目录)
#            /app1 (MHA监控实例根目录)
#                 /manager.log (MHA监控实例日志文件)
mkdir -p /var/log/mha/app1
touch /var/log/mha/app1/manager.log
```





###### 创建独立的监控用户

- 主库用户名，在master mysql的主库执行下列命令建一个新用户 

- 在所有库执行：create user 'mha'@'%' identified by '123456'; 

- 在所有库执行：grant all on *.* to mha@'%' identified by '123456'; 

- 在所有库执行：flush privileges; 



###### 在MHA服务器，配置监控全局配置文件，vim /etc/masterha_default.cnf

```properties
[server default] 
user=mha 
password=123456 
port=3306 

#ssh登录账号 
ssh_user=root 

#从库复制账号和密码 
repl_user=root 
repl_password=123456 
port=3306 

#ping次数 
ping_interval=1 


#二次检查的主机 
secondary_check_script=masterha_secondary_check -s 192.168.207.129 -s 192.168.207.130 -s 192.168.207.131
```



###### 在MHA服务器，配置监控实例配置文件，vim /etc/mha/app1.cnf 

```properties
[server default] 

#MHA监控实例根目录 
manager_workdir=/var/log/mha/app1 


#MHA监控实例日志文件 
manager_log=/var/log/mha/app1/manager.log


#[serverx] 服务器编号 
#hostname 主机名 
#candidate_master 可以做主库 
#master_binlog_dir binlog日志文件目录


[server1] 
hostname=192.168.207.129
candidate_master=1 
master_binlog_dir="/var/lib/mysql"


[server2] 
hostname=192.168.207.130
candidate_master=1 
master_binlog_dir="/var/lib/mysql"


[server3] 
hostname=192.168.207.131
candidate_master=1 
master_binlog_dir="/var/lib/mysql"
```



###### 在MHA Manager服务器上执行ssh通信检测 

```sh
masterha_check_ssh --conf=/etc/mha/app1.cnf
```



###### 在MHA Manager服务器上执行配置检查

出现“MySQL Replication Health is OK.”证明MySQL复制集群没有问题。

```sh
masterha_check_repl --conf=/etc/mha/app1.cnf
```





#### 6）MHA Manager启动

###### 在MHA Manager服务器上执行：

```sh
nohup masterha_manager --conf=/etc/mha/app1.cnf --remove_dead_master_conf --ignore_last_failover < /dev/null > /var/log/mha/app1/manager.log 2>&1 &
```



###### 查看监控状态命令如下

```sh
masterha_check_status --conf=/etc/mha/app1.cnf
```



###### 查看监控日志命令如下：

```sh
tail -f /var/log/mha/app1/manager.log
```







#### 7）测试MHA故障转移（模拟主节点崩溃）

###### 在MHA Manager服务器执行打开日志命令

```sh
tail -500f /var/log/mha/app1/manager.log
```



###### 关闭Master MySQL服务器服务，模拟主节点崩溃

```sh
systemctl stop mysqld
```



###### 查看MHA日志，可以看到哪台slave切换成了master

```
show master status;
```



###### 看到日志中，出现以下内容即表示MHA搭建成功

```log
----- Failover Report -----

app1: MySQL Master failover 192.168.207.130(192.168.207.130:3306) to 192.168.207.129(192.168.207.129:3306) succeeded

Master 192.168.207.130(192.168.207.130:3306) is down!

Check MHA Manager logs at localhost.localdomain:/var/log/mha/app1/manager.log for details.

Started automated(non-interactive) failover.
The latest slave 192.168.207.129(192.168.207.129:3306) has all relay logs for recovery.
Selected 192.168.207.129(192.168.207.129:3306) as a new master.
192.168.207.129(192.168.207.129:3306): OK: Applying all logs succeeded.
192.168.207.131(192.168.207.131:3306): This host has the latest relay log events.
Generating relay diff files from the latest slave succeeded.
192.168.207.131(192.168.207.131:3306): OK: Applying all logs succeeded. Slave started, replicating from 192.168.207.129(192.168.207.129:3306)
192.168.207.129(192.168.207.129:3306): Resetting slave info succeeded.
Master failover to 192.168.207.129(192.168.207.129:3306) completed successfully.
```









#### 8）故障转移主库后，故障恢复手动切换回原主库

###### 启动旧的主库：

```
systemctl start mysqld
```

###### 挂到新主做从库

```shell
change master to master_host='192.168.80.55', master_port=3306, master_user='root',  master_password ='root', master_log_file='xxx', master_log_pos=当前新主节点的日志位置;

start slave; // 开启同步

```

###### 编辑配置文件 /etc/mha/app1.cnf（MHA监控会自动将故障服务器从配置移除）

```
vi   /etc/mha/app1.cnf

#添加节点
```

###### 使用MHA在线切换命令将原主切换回来

结束MHA Manager进程：

```sh
	masterha_stop --global_conf=/etc/masterha/masterha_default.conf --conf=/etc/mha/app1.cnf	
```

执行切换命令：

**注意：**master如果使用ip地址，会报错： 	[error][/usr/share/perl5/vendor_perl/MHA/ServerManager.pm, ln1218] 192.168.80.128 is not alive! 需要将ip地址设置成主机名即可

```sh
masterha_master_switch --conf=/etc/mha/app1.cnf --master_state=alive --new_master_host=master --new_master_port=3306 --orig_master_is_new_slave --running_updates_limit=10000
```

