[toc]



## 两阶段提交协议（2PC）

2PC将事务分成两个阶段，一阶段发起事务询问（执行事务并不提交），二阶段发起提交请求

![image-20210825152445880](images/image-20210825152445880.png)

#### 1. 执行成功

![image-20210825153524444](images/image-20210825153524444.png)

#### 2. 执行失败

![image-20210825153536635](images/image-20210825153536635.png)

#### 3. 优缺点

优点：

- 原理简单

缺点

- 协调器单点问题

  协调者出现故障，将导致整个分布式系统的事务失效。无法发起一二阶段操作或者在一二阶段中途挂掉，将导致分布式环境数据不一致

## 三阶段提交协议（3PC）

 3PC，全称 “three phase commit”，是 2PC 的改进版，将 2PC 的 “提交事务请求” 过程一分为二，共形成了由CanCommit、PreCommit和doCommit三个阶段组成的事务处理协议

![image-20210825154631148](images/image-20210825154631148.png)

#### 1. 与2PC的比较

- 首先对于协调者和参与者都设置了超时机制（在2PC中，只有协调者拥有超时机制，即如果在一定时间内没有收到参与者的消息则默认失败）,主要是避免了参与者在长时间无法与协调者节点通讯（协调者挂掉了）的情况下，无法释放资源的问题，因为参与者自身拥有超时机制会在超时后，自动进行本地commit从而进行释放资源。而这种机制也侧面降低了整个事务的阻塞时间和范围。

- 通过**CanCommit、PreCommit、DoCommit**三个阶段的设计，相较于2PC而言，多设置了一个**缓冲阶段**保证了在最后提交阶段之前各参与节点的状态是一致的 。 

- PreCommit是一个缓冲，保证了在最后提交阶段之前各参与节点的状态是一致的。

#### 2. 问题

- 协调器单点问题

  虽然3PC相对于2PC在参与者上也这是了超时机制，保证协调者挂掉时参与者可主动释放资源。但整体还是依赖于协调者来保证分布式系统的一致性事务。协调者挂了，导致整个分布式系统的事务失效



## NWR协议

NWR是一种在分布式存储系统中用于控制一致性级别的一种策略。在亚马逊的云存储系统中，就应用NWR来控制一致性。

- N：在分布式存储系统中，有多少份备份数据（分布式系统中N往往不能为1，工业界标准为3）

- W：代表一次成功的更新操作要求至少有w份数据写入成功

- R： 代表一次成功的读数据操作要求至少有R份数据成功读取

#### 1. 原理

NWR值的不同组合会产生不同的一致性效果，当W+R>N的时候，整个系统对于客户端来讲能保证强一致性。

##### 1.1 当W+R > N的时

如果R+W>N,则读取操作和写入操作成功的数据一定会有交集（如图中的Node2），这样就可以保证一定能够读取到最新版本的更新数据，数据的强一致性得到了保证

![image-20210825163445600](images/image-20210825163445600.png)

##### 1.1 当W+R <= N的时

因为成功写和成功读集合可能不存在交集，这样读操作无法读取到最新的更新数值，也就无法保证数据的强一致性。R或者W设置的越大，则系统延迟越大，因为这取决于最慢的备份数据的响应时间。

![image-20210825163755334](images/image-20210825163755334.png)

## Gossip协议(流行病协议)

gossip 协议利用一种随机的方式将信息传播到整个网络中，并在一定时间内使得系统内的所有节点数据一致。Gossip 是一种去中心化思路的分布式协议，解决状态在集群中的传播和状态一致性的保证两个问题

数据通过节点像病毒一样逐个传播。因为是指数级传播，整体传播速度非常快。

#### 1. 原理

Gossip 协议的消息传播方式有两种：反熵传播 和 谣言传播

- 反熵传播

  保证最终、完全的一致。缺点是消息数量非常庞大，且无限制；通常只用于新加入节点的数据初始化。

-  谣言传播

  谣言消息在某个时间点之后会被标记为 removed，并且不再被传播。

  缺点是系统有一定的概率会不一致，通常用于节点间数据增量同步

#### 2. 通信方式

Gossip 协议最终目的是将数据分发到网络中的每一个节点。**根据不同的具体应用场景，网络中两个节点之间存在三种通信方式：推送模式、拉取模式、推/拉模式**

- Push

  节点 A 将数据 (key,value,version) 推送给 B 节点，B 节点更新 A 中比自己新的数据

  ![image-20210825164724107](images/image-20210825164724107.png)

- Pull

  A 仅将数据 (key, version) 推送给 B，B 将本地比 A 新的数据（Key, value, version）推送给 A，A 更新本地

  ![image-20210825164758058](images/image-20210825164758058.png)

- Push/Pull

  与 Pull 类似，只是多了一步，A 再将本地比 B 新的数据推送给 B，B 则更新本地

#### 3. 优缺点

Gossip 协议适合于**AP场景的数据一致性处理**，常见应用有：P2P 网络通信、Redis Cluster、Consul。

优点

- 扩展性强，允许任意节点新增/删除
- 去中心化，只要网络连通，任意节点可把消息散播到全网

- 最终一致性，信息指数级的快速传播，在有新信息需要传播时，消息可以快速地发送到全局节点

缺点

- 消息延迟
- 消息冗余

## Paxos协议（只是理论）

Paxos协议就是Paxos算法, Paxos算法是基于==**消息传递**==且具有==**高度容错特性**==的==**一致性算法**==，是目前公认的==**解决分布式一致性问题最有效的算法之一**==。

自Paxos问世以来就==**持续垄断了分布式一致性算法**==，Paxos这个名词几乎等同于分布式一致性。Google的很多大型分布式系统都采用了Paxos算法来解决分布式一致性问题，如Chubby、Megastore以及Spanner等。开源的==**ZooKeeper**==，以及MySQL 5.7推出的用来取代传统的==**主从复制的MySQL**== GroupReplication等纷纷采用Paxos算法解决分布式一致性问题。然而，Paxos的最大特点就是难，不仅难以理解，更难以实现

#### 1. Paxos解决什么问题

Paxos算法解决的问题是上述多个协议无法解决的问题，即分布式系统发生诸如**机器宕机**或**网络异常**（包括消息的延迟、丢失、重复、乱序，还有网络分区）等情况，从而导致无法实现事务一致性的问题

#### 2. 解决方案

**==引入协调者集群，并设置一个主协调者==**

Paxos的版本有: Basic Paxos , Multi Paxos, Fast-Paxos, 具体**==落地有Raft 和zk的ZAB协议==**

![image-20210825172508659](images/image-20210825172508659.png)

#### 3. Basic-Paxos

##### 3.1 Basic-Paxos模型

![image-20210825172745027](images/image-20210825172745027.png)

##### 3.2 basic paxos流程

- Prepare（Proposer第一次请求）

  Proposer提出一个提案,编号为N, 此N大于这个Proposer之前提出所有提出的编号, 请求Accpetor的多数人接受这个提案

- Promise

  如果编号N大于此Accpetor之前接收的任意提案编号则接收并响应给Proposer， 否则拒绝

- Accept（Proposer第二次请求）

  如果达到多数派, Proposer会发出accept请求, 此请求包含提案编号和对应的内容

- Accepted

  如果此Accpetor在此期间没有接受到任何大于N的提案，则接收此提案内容, 否则忽略

##### 3.3 无故障的basic paxos时序图

1. Client发起一个修改V=1的请求
2. Proposer接收请求，发起一个编号=1的提议给Accpetor集群
3. Accpetor接收到提议并与上次的提议编号（初始为0）进行比较，若大于上次并且多数Accpetor同意，则Accpetor将请求提议保存
4. Proposer发起第二次请求，携带提议编号和修改内容给Accpetor集群
5. Accpetor比较提议编号是否为为最新的，是的话则接收修改内容

<img src="images/image-20210825173414279.png" alt="image-20210825173414279" style="zoom:150%;" />

##### 3.4 Acceptor失败时的basic paxos时序图

在下图中，多数派中的一个Acceptor发生故障，因此多数派大小变为2。在这种情况下，BasicPaxos协议仍然成功。

<img src="images/image-20210825173454460.png" alt="image-20210825173454460" style="zoom:150%;" />

##### 3.5 Proposer失败时的basic paxos时序图

Proposer在提出提案之后但在达成协议之前失败。具体来说，传递到Acceptor的时候失败了,这个时候需要选出新的Proposer（提案人）,那么 Basic Paxos协议仍然成功

<img src="images/image-20210825173523326.png" alt="image-20210825173523326" style="zoom:150%;" />

##### 3.6 当多个提议者发生冲突时的basic Paxos

最复杂的情况是多个Proposer都进行提案,导致Paxos的活锁问题

**针对活锁问题解决起来非常简单：只需要在每个Proposer再去提案的时候随机加上一个等待时间即**

**可**

<img src="images/image-20210825173552296.png" alt="image-20210825173552296" style="zoom:150%;" />

#### 4. Multi-Paxos

针对basic Paxos是存在一定得问题,首先就是流程复杂,实现及其困难, 其次效率低(达成一致性需要2轮RPC调用),针对basic Paxos流程进行拆分为选举和复制的过程. 

##### 4.1 第一次流程-确定Leader

- Proposer还是会进行两次RPC调用Acceptor，目的是给Acceptor提供一个Leader选举的时机
- 事务的状态和修改由Leader决定，其他Acceptor附议和复制修改结果

<img src="images/image-20210825173736245.png" alt="image-20210825173736245" style="zoom:150%;" />

##### 4.2 第二次流程-直接由Leader确认

发起过一次事务提交请求后，由于在第一次请求中确认了Leader，所以后续的请求直接由Leader决定

<img src="images/image-20210825173747139.png" alt="image-20210825173747139" style="zoom:150%;" />



##### 4.3 Multi-Paxos角色重叠流程图

Multi-Paxos在实施的时候会将Proposer，Acceptor和Learner的角色合并统称为“服务器”。因此，最后只有“客户端”和“服务器”。

<img src="images/image-20210825173854646.png" alt="image-20210825173854646" style="zoom:150%;" />





## Raft协议（Paxos的具体实现）

引入主节点，通过竞选确定主节点。节点类型：**Follower、Candidate 和 Leader**

**==Leader 会周期性的发送心跳包给 Follower==**。每个 Follower 都设置了一个随机的竞选超时时间，一般为 150ms~300ms，如果**==在这个时间内没有收到 Leader 的心跳包，就会变成 Candidate==**，进入竞选阶段, **==通过竞选阶段的投票多的人成为Leader==**

![image-20210825230634032](images/image-20210825230634032.png)

#### 1. 节点状态

- Leader（主节点）

  接受 client 更新请求，写入本地后，然后同步到其他副本中

- Follower（从节点）

  从 Leader 中接受更新请求，然后写入本地日志文件。对客户端提供读请求

- Candidate（候选节点）

  如果 follower 在一段时间内未收到 leader 心跳。则判断 leader可能故障，发起选主提议。节点状态从 Follower 变为 Candidate 状态，直到选主结束

- termId（任期号）

  每次选举后都会产生一个新的 termId，一个任期内只有一个 leader

#### 2. 竞选阶段流程 

这个是Raft完整版http://thesecretlivesofdata.com/raft/动画演示

github也提供一个https://raft.github.io/动画演示地址 . 原理都是一样的.

1. 分布式系统的最初阶段，只有 Follower，没有 Leader。**==Follower A 等待一个随机的竞选超时时间之后==**，没收到 Leader 发来的心跳包，**==最先进入竞选阶段==**。
2.  最先进入竞选阶段的 **==A 发送投票请求给其它所有节点==**
3. 其它节点会对请求进行回复，如果**==超过一半的节点回复了==**，那么 **==Candidate A就会变成 Leader==**
4. 之后 Leader 会周期性地发送心跳包给 Follower，**==Follower 接收到心跳包==**，**==会重新开始计时==**

#### 3. 多个 Candidate 竞选

1. 如果有多个 Follower 成为 Candidate，并且所获得票数相同，那么就需要重新开始投票

2. 当重新开始投票时，由于每个节点设置的随机竞选超时时间不同，因此能下一次再次出现多个Candidate 并获得同样票数的概率很低

#### 4. 日志复制

1. 来自客户端的修改都会被传入 Leader。注意该修改还未被提交，只是写入日志中
2. Leader 会把修改复制到所有 Follower
3. Leader 会等待大多数的 Follower 也进行了修改，收到大多数反馈后，然后才将修改提交
4. Leader 会通知的所有 Follower 让它们也提交修改，此时所有节点的值达成一致

#### 5. 网络分区

1. 最初始正常情况下状态,Leader B节点会对其他4个节点发送心跳
2. 集群出现网络分区情况，Leader B只能对 A 发送心跳，同时其他三个节点会再次选出一个leader节点
3. 网络分区导致集群脑裂，不过少数派的集群将无法提交事务，多数派可正常工作

#### 6. 网络分区后恢复

1. 出现网络分区，不同集群写入数据不同，但只有多数派的集群可正常提交事务
2. 网络恢复后，Termid 最大的Leader成为新集群的Leader，重新同步节点数据，达成数据一致性

## Lease机制

Lease机制，翻译过来即是租约机制，是一种在分布式系统常用的协议

Lease机制有以下几个特点：

- Lease是颁发者对一段时间内数据一致性的承诺；

- 颁发者发出Lease后，不管是否被接收，只要Lease不过期，颁发者都会按照协议遵守承诺；

- Lease的持有者只能在Lease的有效期内使用承诺，一旦Lease超时，持有者需要放弃执行，重新申请Lease

![image-20210825234827506](images/image-20210825234827506.png)

#### 1. Lease的原理

##### 1.1 引入中心节点负责下发Lease

![image-20210826000220936](images/image-20210826000220936.png)

##### 1.2 出现网络问题

在01:05期间如果出现网络抖动导致其他节点申请Lease会申请失败, 因为中心节点在01:10之前都会承认有主节点,不允许其他节点在申请Lease

![image-20210826000229742](images/image-20210826000229742.png)

##### 1.3 如果网络恢复

![image-20210826000246734](images/image-20210826000246734.png)

##### 1.4 如果到01:10时间,主节点会进行续约操作,然后在下发新的Lease

![image-20210826000310314](images/image-20210826000310314.png)

##### 1.5 如果主节点宕机,其他节点申请Lease也会失败,承认主节点存在

![image-20210826000332461](images/image-20210826000332461.png)

##### 1.6 副节点申请Lease,申请成功. 因为Lease过期

![image-20210826000349251](images/image-20210826000349251.png)



#### 2. lease的容错

**==lease时间长短一般取经验值1-10秒即可。太短网络压力大，太长则收回承诺时间过长影响可用性。==**

- 主节点宕机

  lease机制天生即可容忍网络、lease接收方的出错，时间即Lease剩余过期时长

- 中心节点异常

  颁发者宕机可能使得全部节点没有lease，系统处于不可用状态，解决的方法就是使用一个小集群而不是单一节点作为颁发者。

- 时差问题

  中心节点与主节点之间的时钟可能也存在误差，只需要中心节点考虑时钟误差即可

#### 3. 应用

- GFS(Google 文件系统)

  Master通过lease机制决定哪个是主副本，lease在给各节点的心跳响应消息中携带。收不到心跳时，则等待lease过期，再颁发给其他节点。

- chubby

  paxos选主后，从节点会给主颁发 lease，在期限内不选其他节点为主。另一方面，主节点给每个client节点发送lease，用于判断client死活。