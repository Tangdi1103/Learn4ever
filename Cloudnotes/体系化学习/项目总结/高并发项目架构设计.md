[toc]

## 一、当前项目

### 1. 项目名称

某航空公司商务移动应用平台

会员人数达到6000W+

平均每日 PV 过亿

峰值QPS 2W+

### 2. 项目的业务背景

移动互联网盛行的背景下，南航为了全面实施数字化转型、打造世界一流航空运输企业的战略举措，旨在通过南航开发运营的移动端官方平台，为旅客提供全流程电子化服务，比如机票预订、退票、行程管理、航班动态查询、个人中心管理等一站式服务的移动应用平台，支持APP、小程序、触屏等客户端。实现一机在手，全程无忧。



### 3. 项目的核心架构图

![image-20220216013121308](images/image-20220216013121308.png)

### 4. 你在该项目中的职责、任务？你所负责的部分用到了哪些技术？以及有哪些亮点？

我负责聚合业务服务中的，机票预订产品的开发。该产品下又细分机票查询、订单、支付等模块，技术选型为SpringBoot、SpringMVC、SpringCloud（Nacos、Feign、Hystrix、GateWay、Sleuth、Zipkin）、Redis、MySQL、RocketMQ、Motan





##### 问题1：用户支付后，反应订单的支付状态有延迟，体验不好

- 描述

  用户支付成功后，**支付服务通过异步回调我们的接口**，通知我们支付结果。然后我们**再进行订单号流水号的合法性校验**、**订单状态的校验等业务规则处理**，再**调用订单服务修改状态**、**短信服务发送短信**、**用户服务增加积分**。异步回调接口中有大量同步业务的处理以及频繁的网络IO，导致接口的TPS较低，当流量高时，有部分用户的支付状态同步过长。

- 解决方案

  将接口的短信服务、用户服务的调用砍掉，新增MQ异步回调的技术方案。支付服务通过MQ分发回调通知，各微服务异步消费。

##### 问题2：2020年疫情时，国际票价翻了好几倍 ，黄牛恶意抢票

- 描述

  系统监控日志发现流量异常（查票接口有规律的5-6W的QPS），定位到应该是有黄牛使用爬虫程序恶意扣票。就是订票不支付，然后在订单超时之前自动取消订单同时又将这张票订下。他们有很多个账号，之前好像检测中的有几百个了，交替反复的进行上述操作，大概5到6账号维护一张票，规避了系统针对单个账号频繁订票的防占座机制。同时发现还有大批量异常活动的IP，即使经过nginx限流依然还能保持2k+的额外qps，并且经常变更请求的频率。

- 解决方案

  我们在网关层增加了拦截策略

  - 针对读服务，**IP和账号**请求频率可疑的，一小时内超过100次的，提高限流阈值为10分钟一次。（由于nginx限流后单个ip不超过2~3个每秒，所以没必要使用setnxex锁。直接exist然后setex或incr即可）

    这次操作直接降低大部分读服务的异常流量，一天的PV直接降低5000W~6000W

  - 分析账号行为，针对订票和退票的请求频率超过6次每3小时的，弹出验证码进行人机识别。
  - 针对订票的接口，IP请求频率可疑的，无法发现。因为谁也不知道什么时候有票，对方也只有读到有票时才并发去抢票。并且扣票的话完全可以10分钟一次

##### 问题3：

采用GuavaCache替代ConcurrentHashMap作为本地缓存

### 5. 相关数据指标、性能指标要求？具体是什么？

> 高性能：查票、下单和支付业务，对于TP90指标 要求为 <600ms
>
> 高可用：>99.9%
>
> 高并发：查票QPS > 50W，订单业务的TPS >5000





## 二、流量高峰-秒杀场景

### 1. 秒杀系统功能需求

#### 特性

- 时间短
- 库存少
- 并发高

#### 业务流程图

![image-20220217010550845](images/image-20220217010550845.png)

#### 前端交互逻辑

![image-20220217010527756](images/image-20220217010527756.png)

#### 后端需求分析

- 秒杀活动页需求的秒杀活动信息列表的接口，活动开始时间、结束
- 时间、商品列表。对应提醒按钮需求的Push订阅接口
- 对应商品详情页需求的商品活动接口信息
- 对应秒杀按钮需求的秒杀抢购接口
- 获得商品信息，对接商品中心
- 商品库存信息，对接库存中心
- 抢购生成订单，对接交易中心

#### 管理后台需求

- 活动场次信息管理：场次时间、商品种类
- 活动商品信息管理：商品数量、种类、型号、折扣等
- 缓存管理

#### 非功能需求

非功能需求是**系统在特定条件下正常运行的最低要求**。在系统的设计之初，我们就应当给这些最低要求赋予明确的定义，**给出明确的指标**，为了分析，可将影响要素分为与系统运行环境有关的**内部因素与系统运行环境无关的外部因素**

- 高可用指标，如，可用性方面要高于 99.99%
- 高性能指标，如，高性能方面要求请求延迟小于 200ms（TP90 200ms）
- 高并发指标，如，高并发方面要求QPS 大于 10万，订单TPS达到20000
- 安全防护能力
- 运营/运维成本

#### E-R图

![image-20220217012948003](images/image-20220217012948003.png)



### 2. 秒杀系统架构设计

#### 面对的挑战

- 对现有的**业务冲击**

  秒杀**拆分为独立服务**（也包括页面）

- 高并发下的**服务和数据库的压力**

  - **多级缓存**，CDN存储大量静态资源，本地缓存热卖商品详情，Redis分布式缓存非热卖商品详情。缓存一致性可采用主动更新策略：数据库更新，canal发送增量日志到MQ，消费者更新缓存数据

  - **限流**，将**写请求拦截在系统上游**（有效请求非常低的情况）

  - 数据库**使用分库分表和读写分离**

- **库存超卖**

  - 通过**悲观锁或者乐观锁控制库存扣减**，防止超卖；若库存够大，可将库存进行分片到多个库中。然后通过uid进行路由到对应库存库，监控库存剩余5%时，将分片回收到主片中。并且由分布式事务来控制主片和分片的库存回收。

  - **在Redis中扣减**（库存判断及扣减等操作需要使用Lua脚本完成，保证操作的原子性隔离性），然后将消息写入MQ，**异步更新数据库**

    **缺点：**若**Redis挂了重启，则可能会造成部分数据的丢失**。此时必然需要将库存从数据库中**重新加载到Redis中**，而此时数据库**消费了MQ中的扣减记录，造成Redis中的库存大于数据库**，可能导致超卖

- 如果使用了缓存，还需要注意：

  - **缓存与数据库的一致性**

    - 主动更新

      更新数据库后，通过Canal中间件将binlog日志采集发送到MQ中，然后更新缓存

    - 被动更新

      被动或主动删除缓存（设置缓存key的ttl 或者 写数据库时删除缓存），然后在**读时，查数据库回填数据到缓存**，这属于被动更新

      **注意：**针对热点Key的高并发场景，采取**延时双删策略**。读时可能造成**缓存击穿**，可以采用**分布式锁控制一个请求去查库**，剩余请求**阻塞等待缓存更新，可设置等待超时时间**

  - **怎么防止缓存雪崩、缓存击穿、缓存穿透的**

  - 如果有**事先未预料到的商品突然成为热点，流量突然激增**，有什么应急措施。

    应急措施：对该商品的**限流阈值往下调（高并发系统必然有多种限流机制）**。

    正确做法：通过大数据的流式计算平台，如Flink对商品访问次数进行实时统计。当达到某个阈值时，进行**自动限流**，并将该商品**加载到本地缓存**，并开启主动更新策略或被动更新策略，**然后解除限流**

- 订单未支付

  若业务要求超时订单**尽量准时自动取消**，则可利用MQ的**延时队列和死信队列**。将订单放入延时队列，设置延时时间，时间一到调用死信交换器将消息放入死信队列，然后将死信队列的订单进行回滚操作

  ![image-20220304135959953](images/image-20220304135959953.png)

- 如果使用MQ，还需要注意：

  消息丢失，消息堆积，消息重复。虽然MQ自身有机制来保证各环节的消息一致性，防止消息丢失和消息重复。但网络传输总会有问题，生产方无法确保消息是否被消费方消费，

  解决方案：创建一个消息表，使用递增的全局唯一分布式ID对消息编号并标识状态。可以防止消息丢失和重复

  ![image-20220304142647325](images/image-20220304142647325.png)

- 下游接口故障，如何降级处理

  下游接口出现异常，可采用**有损、异步的降级处理方案**

  - 比如库存扣减接口异常，可先生成订单，但是提醒用户正在出库，但是有可能无库存需要排队
  - 比如视频/文章风控校验接口异常，可先发布但只能自己看，通过异步方式进行校验（如MQ分发、人工等）

- 进攻与防守

  - **一个账号**发起**大量并发请求**

    如领取奖励的逻辑，高并发的场景下，一个请求领取积分成功然后修改数据库记录，此时在这之间该用户还有上百个该请求，绕过了判断逻辑，重复领取积分

    使用Redis分布式锁，或者使用Redis事务watch的乐观锁特性

  - **大量“僵尸账号”**发起**大量并发请求**

    检测IP请求频率，对其进行限流或者使用验证码防爬

  - **大量“僵尸账号”**发起**不同IP**的**大量并发请求**

    这些人通过随机IP代理服务，发起大量不同ip的请求。只能通过设置业务门槛（如限制用户等级、活跃度、资料完善度等），来解决掉这部分的流量攻击

#### 架构设计

![image-20220217015051971](images/image-20220217015051971.png)

![image-20220217014908809](images/image-20220217014908809.png)

![秒杀](images/秒杀.png)

前端层

- 秒杀页面展示。
- 倒计时，与时间服务器同步时间，未到点不可点击。
- 请求拦截，防止重复大量的写请求。

站点层

- 页面的静态资源则走CDN。

- 根据ip频率和账号uid的请求次数进行限流，验证码来人机识别

服务层

- 商品详情等信息读缓存，当数据库商品信息修改，则通过 canal->MQ 更新缓存数据。
- 库存扣减、生成订单（以下根据业务场景二选一）
  - **库存扣减、秒杀记录走缓存**，然后发布消息到MQ**异步更新到数据库**，订单生成调用订单服务、营销服务扣减优惠券、短信服务发送短信。
  
    **缺点：**若**Redis挂了重启，则可能会造成部分数据的丢失**。此时必然需要将库存从数据库中**重新加载到Redis中**，而此时数据库**消费了MQ中的扣减记录，造成Redis中的库存大于数据库**，可能导致超卖。
  
  - 通过**悲观锁或者乐观锁控制库存扣减**，防止超卖；若库存够大，可将库存进行分片到多个库中。然后通过uid进行路由到对应库存库，监控库存剩余5%时，将分片回收到主片中。并且由分布式事务来控制主片和分片的库存回收。
